{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/B-lilily/Student-Performance-Analysis/blob/main/ml-app-gradio/3_Speech-2-text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ZjpGTr2kjV"
      },
      "source": [
        "## Automatic Speech Recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-a-bb-Z2kjW"
      },
      "source": [
        "What is speech to text?\n",
        "\n",
        "- Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text.\n",
        "[Reference](https://huggingface.co/tasks/automatic-speech-recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8bjDOb2kjW"
      },
      "source": [
        "##### Thonburian Whisper\n",
        "\n",
        "https://huggingface.co/biodatlab/whisper-th-medium-combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QhD6yOX2kjW"
      },
      "source": [
        "### Step 1: Install Transformers\n",
        "Install the Hugging Face Transformers library so we can use the Whisper speech to text pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL1nFRBG2kjW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05PCACvw2kjX"
      },
      "source": [
        "### Step 2: Load the ASR model and pipeline\n",
        "Import Transformers and PyTorch, pick a Whisper model, choose CPU or GPU, and configure Thai transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfjQFkj32kjX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"biodatlab/whisper-th-medium-combined\"  # specify the model name\n",
        "lang = \"th\"  # change to Thai langauge\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\", # specify the task\n",
        "    model=MODEL_NAME, # define the model\n",
        "    chunk_length_s=30, # specify the chunk length\n",
        "    device=device, # use GPU if available\n",
        ")\n",
        "\n",
        "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(\n",
        "  language=lang,\n",
        "  task=\"transcribe\" # specify the task\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFY6GAw2kjY"
      },
      "source": [
        "### Optional: Install pytubefix\n",
        "Only needed if you want to download a YouTube audio sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLNp3OU42kjY"
      },
      "outputs": [],
      "source": [
        "!pip install pytubefix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APehD-dL2kjY"
      },
      "source": [
        "### Optional: Download a sample audio file\n",
        "Fetch audio from YouTube, save it as an MP3 in `audio_example/`, and use the repo sample if the download fails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMq5cXnO2kjZ"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "from pytubefix import YouTube\n",
        "import os\n",
        "\n",
        "yt = YouTube(\"https://www.youtube.com/EXAMPLEURL\")  #\n",
        "## if you can't use this code, please use the example audio file in the repo\n",
        "\n",
        "video = yt.streams.filter(only_audio=True).first()\n",
        "os.makedirs(\"audio_example\", exist_ok=True)\n",
        "out_file = video.download(output_path=\"audio_example\")\n",
        "\n",
        "new_file = \"audio_example/audio\" + '.mp3'\n",
        "os.rename(out_file, new_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iblU1_am2kjZ"
      },
      "source": [
        "### Step 3: Transcribe the audio file\n",
        "Run the pipeline on the MP3 file. This can take a while on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHUpyVqz2kjZ"
      },
      "outputs": [],
      "source": [
        "text = pipe(\"audio_example/audio.mp3\")[\"text\"] # give audio mp3 and transcribe text\n",
        "## this would take a while to process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQPnNYdR2kjZ"
      },
      "source": [
        "### Step 4: View the transcription\n",
        "Print the text output so you can inspect the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNfTMrmL2kjZ"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbT5x2Q22kjZ"
      },
      "source": [
        "### Bonus: Build a Gradio demo\n",
        "Create a small web UI that records microphone audio and returns the transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzJFIP6v2kjZ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def transcribe_audio(audio):\n",
        "    return pipe(audio)[\"text\"]\n",
        "\n",
        "mic_input = gr.Audio(type=\"filepath\", label=\"Speak into the microphone\")\n",
        "text_output = gr.Textbox(label=\"Transcribed Text\")\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=mic_input,\n",
        "    outputs=text_output,\n",
        "    title=\"Speech-to-Text Transcription\",\n",
        "    description=\"Speak into the microphone and get the transcribed text.\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}